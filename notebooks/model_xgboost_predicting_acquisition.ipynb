{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3sJnuNGjLKp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JMxANwlCZpc"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JH2PeWgyCb4d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, balanced_accuracy_score,  classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "def show_ConfusionMatrix_test(y_test, y_test_pred, test_confusion_matrix_title = \"Confusion Matrix (Test)\"):\n",
        "  conf_matrix_log_reg = confusion_matrix(y_test, y_test_pred)\n",
        "  print(\"Confusion Matrix (Test):\")\n",
        "  print(conf_matrix_log_reg)\n",
        "  ConfusionMatrixDisplay(conf_matrix_log_reg).plot()\n",
        "  plt.title(test_confusion_matrix_title)\n",
        "  plt.show()\n",
        "'''\n",
        "def find_optimal_threshold(target, predicted):\n",
        "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
        "    Parameters:\n",
        "    target : Matrix with dependent or target data, where rows are observations\n",
        "    predicted : Matrix with predicted data, where rows are observations\n",
        "\n",
        "    Returns:\n",
        "    list type, with optimal cutoff value\n",
        "    \"\"\"\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    i = np.arange(len(tpr))\n",
        "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
        "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
        "    return list(roc_t['threshold'])\n",
        "'''\n",
        "\n",
        "def get_precision_and_recall(y, y_pred):\n",
        "  return round(precision_score(y, y_pred), 4), round(recall_score(y, y_pred), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiQiTwWWjY1a"
      },
      "source": [
        "# Download dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coIsc_hajY-3",
        "outputId": "dfa74199-b899-459d-9675-1d1b72e16569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /home/mayavb/anaconda3/lib/python3.11/site-packages (0.2.41)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (4.9.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (2023.3.post1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (4.12.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /home/mayavb/anaconda3/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tweepy in /home/mayavb/anaconda3/lib/python3.11/site-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from tweepy) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: wbdata in /home/mayavb/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: pandas in /home/mayavb/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
            "Requirement already satisfied: appdirs<2.0,>=1.4 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (1.4.4)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (2.2.1)\n",
            "Requirement already satisfied: cachetools<6.0.0,>=5.3.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (5.4.0)\n",
            "Requirement already satisfied: dateparser<2.0.0,>=1.2.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (1.2.0)\n",
            "Requirement already satisfied: decorator<6.0.0,>=5.1.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (5.1.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (2.31.0)\n",
            "Requirement already satisfied: shelved-cache<0.4.0,>=0.3.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (0.3.1)\n",
            "Requirement already satisfied: tabulate<0.9.0,>=0.8.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from wbdata) (0.8.10)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2023.10.3)\n",
            "Requirement already satisfied: tzlocal in /home/mayavb/anaconda3/lib/python3.11/site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.0->wbdata) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.0->wbdata) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.0->wbdata) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mayavb/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.0->wbdata) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install Kaggle API\n",
        "# !pip install kaggle\n",
        "\n",
        "# # Make a directory for the Kaggle credentials and move the JSON file there\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# # Set permissions for the JSON file\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !cd ../data/\n",
        "\n",
        "# # Download the dataset\n",
        "# !kaggle datasets download -d manishkc06/startup-success-prediction\n",
        "\n",
        "# # Unzip the downloaded file\n",
        "# !unzip startup-success-prediction.zip\n",
        "\n",
        "# !ls\n",
        "%pip install yfinance\n",
        "%pip install tweepy\n",
        "%pip install wbdata pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1Rkwo8jhDZ"
      },
      "source": [
        "# Explore and adjust data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jUh80PBqix"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFXKW9TEsb_n"
      },
      "source": [
        "### add economic info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1XH9LBT0AW7P"
      },
      "outputs": [],
      "source": [
        "# Affect acquisitions\n",
        "# Define the function to add NASDAQ annual changes\n",
        "# def add_nasdaq_annual_changes(df):\n",
        "#     years = np.arange(1984, 2024).tolist()\n",
        "#     nasdaq_data = yf.download('^IXIC', start='1984-01-01', end='2024-01-01', interval='1mo')\n",
        "#     nasdaq_data['Year'] = nasdaq_data.index.year\n",
        "#     nasdaq_annual = nasdaq_data.groupby('Year')['Close'].last().pct_change().reset_index()\n",
        "#     nasdaq_annual.columns = ['Year', 'NASDAQ_Annual_Change']\n",
        "#     nasdaq_annual = nasdaq_annual[nasdaq_annual['Year'].isin(years)]\n",
        "#     nasdaq_annual.set_index('Year', inplace=True)\n",
        "\n",
        "#     for i in range(11):\n",
        "#         df[f'nasdaq_annual_changes_at_year_{i}'] = df['founded_at_year'].apply(lambda x: nasdaq_annual['NASDAQ_Annual_Change'].get(x-i, np.nan))\n",
        "\n",
        "#     return df\n",
        "\n",
        "\n",
        "def add_nasdaq_annual_changes(df):\n",
        "    years = np.arange(1984, 2024).tolist()\n",
        "    nasdaq_data = yf.download('^IXIC', start='1984-01-01', end='2024-01-01', interval='1mo')\n",
        "    nasdaq_data['Year'] = nasdaq_data.index.year\n",
        "    nasdaq_annual = nasdaq_data.groupby('Year')['Close'].last().pct_change().reset_index()\n",
        "    nasdaq_annual.columns = ['Year', 'NASDAQ_Annual_Change']\n",
        "    nasdaq_annual = nasdaq_annual[nasdaq_annual['Year'].isin(years)]\n",
        "    nasdaq_annual.set_index('Year', inplace=True)\n",
        "    \n",
        "    # Add columns for each year based on the maximum founded_at_year in the DataFrame\n",
        "    max_year = df['founded_at_year'].max()\n",
        "    for year in range(max_year, max_year - 11, -1):\n",
        "        year_index = max_year - year\n",
        "        df[f'nasdaq_annual_changes_at_year_{year_index}'] = df['founded_at_year'].apply(\n",
        "            lambda x: nasdaq_annual.loc[year, 'NASDAQ_Annual_Change'] if year in nasdaq_annual.index else np.nan\n",
        "        )\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Affect workforce and investments\n",
        "def add_info_on_inflation_over_years(df):\n",
        "  us_inflation_data = {\n",
        "      1984: 4.3, 1985: 3.6, 1986: 1.9, 1987: 3.6, 1988: 4.1, 1989: 4.8,\n",
        "      1990: 5.4, 1991: 4.2, 1992: 3.0, 1993: 2.7, 1994: 2.6, 1995: 2.8,\n",
        "      1996: 3.0, 1997: 2.3, 1998: 1.6, 1999: 2.2, 2000: 3.4, 2001: 2.8,\n",
        "      2002: 1.6, 2003: 2.3, 2004: 2.7, 2005: 3.4, 2006: 3.2, 2007: 2.8,\n",
        "      2008: 3.8, 2009: -0.4, 2010: 1.6, 2011: 3.2, 2012: 2.1, 2013: 1.5,\n",
        "      2014: 1.6, 2015: 0.1, 2016: 1.3, 2017: 2.1, 2018: 2.4, 2019: 1.8,\n",
        "      2020: 1.2, 2021: 4.7, 2022: 8.0, 2023: 3.2, 2024: 2.5  # 2024 is an estimate\n",
        "  }\n",
        "\n",
        "  df['inflation_at_year_0'] = df['founded_at_year'].map(us_inflation_data)\n",
        "  df['inflation_at_year_1'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-1, None))\n",
        "  df['inflation_at_year_2'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-2, None))\n",
        "  df['inflation_at_year_3'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-3, None))\n",
        "\n",
        "  df['inflation_at_year_4'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-4, None))\n",
        "  df['inflation_at_year_5'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-5, None))\n",
        "  df['inflation_at_year_6'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-6, None))\n",
        "  df['inflation_at_year_7'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-7, None))\n",
        "  df['inflation_at_year_8'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-8, None))\n",
        "  df['inflation_at_year_9'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-9, None))\n",
        "  df['inflation_at_year_10'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-10, None))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Affect workforce and ivestments\n",
        "def add_us_gdp_growth_data_over_years(df):\n",
        "    us_gdp_growth_data = {\n",
        "        1984: 7.3, 1985: 4.2, 1986: 3.5, 1987: 3.5, 1988: 4.2, 1989: 3.7,\n",
        "        1990: 1.9, 1991: -0.1, 1992: 3.6, 1993: 2.8, 1994: 4.0, 1995: 2.7,\n",
        "        1996: 3.8, 1997: 4.5, 1998: 4.5, 1999: 4.8, 2000: 4.1, 2001: 1.0,\n",
        "        2002: 1.7, 2003: 2.9, 2004: 3.8, 2005: 3.5, 2006: 2.9, 2007: 1.9,\n",
        "        2008: -0.1, 2009: -2.5, 2010: 2.6, 2011: 1.6, 2012: 2.2, 2013: 1.8,\n",
        "        2014: 2.5, 2015: 3.1, 2016: 1.6, 2017: 2.4, 2018: 2.9, 2019: 2.3,\n",
        "        2020: -3.4, 2021: 5.9, 2022: 2.1, 2023: 2.0, 2024: 2.3  # 2024 is an estimate\n",
        "    }\n",
        "\n",
        "    df['gdp_growth_at_year_0'] = df['founded_at_year'].map(us_gdp_growth_data)\n",
        "    df['gdp_growth_at_year_1'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-1, None))\n",
        "    df['gdp_growth_at_year_2'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-2, None))\n",
        "    df['gdp_growth_at_year_3'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-3, None))\n",
        "\n",
        "    df['gdp_growth_at_year_4'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-4, None))\n",
        "    df['gdp_growth_at_year_5'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-5, None))\n",
        "    df['gdp_growth_at_year_6'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-6, None))\n",
        "    df['gdp_growth_at_year_7'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-7, None))\n",
        "    df['gdp_growth_at_year_8'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-8, None))\n",
        "    df['gdp_growth_at_year_9'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-9, None))\n",
        "    df['gdp_growth_at_year_10'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-10, None))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Affect workforce\n",
        "def add_us_workforce_growth_data_over_years(df):\n",
        "    us_workforce_growth_data = {\n",
        "        1984: 1.5, 1985: 1.6, 1986: 1.5, 1987: 1.7, 1988: 1.6, 1989: 1.4,\n",
        "        1990: 1.3, 1991: 1.1, 1992: 1.3, 1993: 1.2, 1994: 1.4, 1995: 1.2,\n",
        "        1996: 1.3, 1997: 1.2, 1998: 1.3, 1999: 1.2, 2000: 1.2, 2001: 0.9,\n",
        "        2002: 0.8, 2003: 0.7, 2004: 1.1, 2005: 1.2, 2006: 1.1, 2007: 1.1,\n",
        "        2008: 0.7, 2009: -0.3, 2010: 0.7, 2011: 0.7, 2012: 1.0, 2013: 0.9,\n",
        "        2014: 1.0, 2015: 1.2, 2016: 1.1, 2017: 1.0, 2018: 1.3, 2019: 1.0,\n",
        "        2020: -2.3, 2021: 2.5, 2022: 1.8, 2023: 1.0, 2024: 1.1  # 2024 is an estimate\n",
        "    }\n",
        "\n",
        "    df['workforce_growth_at_year_0'] = df['founded_at_year'].map(us_workforce_growth_data)\n",
        "    df['workforce_growth_at_year_1'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-1, None))\n",
        "    df['workforce_growth_at_year_2'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-2, None))\n",
        "    df['workforce_growth_at_year_3'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-3, None))\n",
        "\n",
        "    df['workforce_growth_at_year_4'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-4, None))\n",
        "    df['workforce_growth_at_year_5'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-5, None))\n",
        "    df['workforce_growth_at_year_6'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-6, None))\n",
        "    df['workforce_growth_at_year_7'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-7, None))\n",
        "    df['workforce_growth_at_year_8'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-8, None))\n",
        "    df['workforce_growth_at_year_9'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-9, None))\n",
        "    df['workforce_growth_at_year_10'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-10, None))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_economic_indicators(df, indicator_code):\n",
        "    # Define the fixed range of years\n",
        "    years_range = 11\n",
        "    start_year = df['founded_at_year'].min() - years_range\n",
        "    end_year = df['founded_at_year'].max()\n",
        "\n",
        "    # Fetch data from World Bank for a broad range of years\n",
        "    countries = 'US'\n",
        "    data = wbdata.get_dataframe({indicator_code: 'Economic_Indicator'}, country=countries)\n",
        "    \n",
        "    # Reset index and rename columns\n",
        "    data = data.reset_index()\n",
        "    data.rename(columns={indicator_code: 'Economic_Indicator'}, inplace=True)\n",
        "    \n",
        "    # Ensure 'date' column is in datetime format\n",
        "    data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
        "    \n",
        "    # Extract year from the date and filter data\n",
        "    data['Year'] = data['date'].dt.year\n",
        "    data = data.set_index(['Year'])\n",
        "    \n",
        "    # Filter data to include only the years of interest\n",
        "    years_of_interest = np.arange(start_year, end_year + 1)\n",
        "    data = data.loc[data.index.isin(years_of_interest)]\n",
        "    \n",
        "    # Add columns for each year in the fixed range\n",
        "    for i in range(years_range):\n",
        "        if 'GDP' in indicator_code:\n",
        "            key_word = 'GDP'\n",
        "        elif 'UEM' in indicator_code:\n",
        "            key_word = 'UEM'\n",
        "        else:\n",
        "            key_word = 'UNKNOWN'\n",
        "\n",
        "        year = end_year - i\n",
        "        df[f'{key_word}_at_year_{i}'] = df['founded_at_year'].apply(\n",
        "            lambda x: data.loc[year, 'Economic_Indicator'] if year in data.index else np.nan\n",
        "        )\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6dM8K_zsoVT"
      },
      "source": [
        "### add & adjust data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9ficszZWssKj"
      },
      "outputs": [],
      "source": [
        "def read_and_adjust(file_name = 'startup data.csv'):\n",
        "  # Load the data\n",
        "  df = pd.read_csv(file_name)\n",
        "  \n",
        "  # Create lable\n",
        "  df['status_code'] = df['status'].map({'acquired': 1, 'closed': 0})\n",
        "\n",
        "  # Convert the date column to datetime\n",
        "  df['founded_at_date'] = pd.to_datetime(df['founded_at'])\n",
        "\n",
        "  # Extract year, month, and day into separate columns\n",
        "  df['founded_at_year'] = df['founded_at_date'].dt.year\n",
        "  df['founded_at_month'] = df['founded_at_date'].dt.month\n",
        "  df['founded_at_day'] = df['founded_at_date'].dt.day\n",
        "\n",
        "  # Drop colums\n",
        "  df = df.drop(columns=['status', 'founded_at', 'name', 'id', 'state_code', 'object_id', 'category_code', 'labels', 'closed_at'])\n",
        "\n",
        "  #df = add_info_on_inflation_over_years(df)\n",
        "  # df = add_us_gdp_growth_data_over_years(df)\n",
        "  #df = add_us_workforce_growth_data_over_years(df)\n",
        "  \n",
        "  df = add_nasdaq_annual_changes(df)\n",
        "  \n",
        "  indicator_code = 'NY.GDP.MKTP.KD.ZG'\n",
        "  df = add_economic_indicators(df, indicator_code) # will replace add_us_gdp_growth_data_over_years\n",
        "  \n",
        "  indicator_code = 'SL.UEM.TOTL.ZS'  # Unemployment rate, percentage of total labor force\n",
        "  df = add_economic_indicators(df, indicator_code) # will replace add_us_gdp_growth_data_over_years\n",
        "\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B2BTdJJBtyN"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7W7KnxJjljM",
        "outputId": "7b58047f-a380-47d3-f6dc-21c04d6e4839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "# import tweepy\n",
        "import wbdata\n",
        "\n",
        "dataset_path = '../data/startup_data.csv'\n",
        "df = read_and_adjust(file_name = dataset_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "# df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 923 entries, 0 to 922\n",
            "Data columns (total 78 columns):\n",
            " #   Column                            Non-Null Count  Dtype         \n",
            "---  ------                            --------------  -----         \n",
            " 0   Unnamed: 0                        923 non-null    int64         \n",
            " 1   latitude                          923 non-null    float64       \n",
            " 2   longitude                         923 non-null    float64       \n",
            " 3   zip_code                          923 non-null    object        \n",
            " 4   city                              923 non-null    object        \n",
            " 5   Unnamed: 6                        430 non-null    object        \n",
            " 6   first_funding_at                  923 non-null    object        \n",
            " 7   last_funding_at                   923 non-null    object        \n",
            " 8   age_first_funding_year            923 non-null    float64       \n",
            " 9   age_last_funding_year             923 non-null    float64       \n",
            " 10  age_first_milestone_year          771 non-null    float64       \n",
            " 11  age_last_milestone_year           771 non-null    float64       \n",
            " 12  relationships                     923 non-null    int64         \n",
            " 13  funding_rounds                    923 non-null    int64         \n",
            " 14  funding_total_usd                 923 non-null    int64         \n",
            " 15  milestones                        923 non-null    int64         \n",
            " 16  state_code.1                      922 non-null    object        \n",
            " 17  is_CA                             923 non-null    int64         \n",
            " 18  is_NY                             923 non-null    int64         \n",
            " 19  is_MA                             923 non-null    int64         \n",
            " 20  is_TX                             923 non-null    int64         \n",
            " 21  is_otherstate                     923 non-null    int64         \n",
            " 22  is_software                       923 non-null    int64         \n",
            " 23  is_web                            923 non-null    int64         \n",
            " 24  is_mobile                         923 non-null    int64         \n",
            " 25  is_enterprise                     923 non-null    int64         \n",
            " 26  is_advertising                    923 non-null    int64         \n",
            " 27  is_gamesvideo                     923 non-null    int64         \n",
            " 28  is_ecommerce                      923 non-null    int64         \n",
            " 29  is_biotech                        923 non-null    int64         \n",
            " 30  is_consulting                     923 non-null    int64         \n",
            " 31  is_othercategory                  923 non-null    int64         \n",
            " 32  has_VC                            923 non-null    int64         \n",
            " 33  has_angel                         923 non-null    int64         \n",
            " 34  has_roundA                        923 non-null    int64         \n",
            " 35  has_roundB                        923 non-null    int64         \n",
            " 36  has_roundC                        923 non-null    int64         \n",
            " 37  has_roundD                        923 non-null    int64         \n",
            " 38  avg_participants                  923 non-null    float64       \n",
            " 39  is_top500                         923 non-null    int64         \n",
            " 40  status_code                       923 non-null    int64         \n",
            " 41  founded_at_date                   923 non-null    datetime64[ns]\n",
            " 42  founded_at_year                   923 non-null    int32         \n",
            " 43  founded_at_month                  923 non-null    int32         \n",
            " 44  founded_at_day                    923 non-null    int32         \n",
            " 45  nasdaq_annual_changes_at_year_0   923 non-null    float64       \n",
            " 46  nasdaq_annual_changes_at_year_1   923 non-null    float64       \n",
            " 47  nasdaq_annual_changes_at_year_2   923 non-null    float64       \n",
            " 48  nasdaq_annual_changes_at_year_3   923 non-null    float64       \n",
            " 49  nasdaq_annual_changes_at_year_4   923 non-null    float64       \n",
            " 50  nasdaq_annual_changes_at_year_5   923 non-null    float64       \n",
            " 51  nasdaq_annual_changes_at_year_6   923 non-null    float64       \n",
            " 52  nasdaq_annual_changes_at_year_7   923 non-null    float64       \n",
            " 53  nasdaq_annual_changes_at_year_8   923 non-null    float64       \n",
            " 54  nasdaq_annual_changes_at_year_9   923 non-null    float64       \n",
            " 55  nasdaq_annual_changes_at_year_10  923 non-null    float64       \n",
            " 56  GDP_at_year_0                     923 non-null    float64       \n",
            " 57  GDP_at_year_1                     923 non-null    float64       \n",
            " 58  GDP_at_year_2                     923 non-null    float64       \n",
            " 59  GDP_at_year_3                     923 non-null    float64       \n",
            " 60  GDP_at_year_4                     923 non-null    float64       \n",
            " 61  GDP_at_year_5                     923 non-null    float64       \n",
            " 62  GDP_at_year_6                     923 non-null    float64       \n",
            " 63  GDP_at_year_7                     923 non-null    float64       \n",
            " 64  GDP_at_year_8                     923 non-null    float64       \n",
            " 65  GDP_at_year_9                     923 non-null    float64       \n",
            " 66  GDP_at_year_10                    923 non-null    float64       \n",
            " 67  UEM_at_year_0                     923 non-null    float64       \n",
            " 68  UEM_at_year_1                     923 non-null    float64       \n",
            " 69  UEM_at_year_2                     923 non-null    float64       \n",
            " 70  UEM_at_year_3                     923 non-null    float64       \n",
            " 71  UEM_at_year_4                     923 non-null    float64       \n",
            " 72  UEM_at_year_5                     923 non-null    float64       \n",
            " 73  UEM_at_year_6                     923 non-null    float64       \n",
            " 74  UEM_at_year_7                     923 non-null    float64       \n",
            " 75  UEM_at_year_8                     923 non-null    float64       \n",
            " 76  UEM_at_year_9                     923 non-null    float64       \n",
            " 77  UEM_at_year_10                    923 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(40), int32(3), int64(28), object(6)\n",
            "memory usage: 551.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfCxLxAw-1aJ",
        "outputId": "6d8d595b-7f17-4794-9b3b-141b6b93aee0"
      },
      "outputs": [],
      "source": [
        "df.founded_at_year.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbNsKVg5jhOM",
        "outputId": "52c67b88-cf3a-48de-e3af-72b738667c94"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFnaN8kKj9zQ",
        "outputId": "7928b7c6-742a-418a-f787-a9512d3a0a64"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtaYf4umO6L"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06pLkTuZEoR"
      },
      "source": [
        "### Number of components needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "3c_12tPz69Pv",
        "outputId": "5dd63f51-2227-4894-904f-b58c15bd42ac"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "thereshold_PCA = 0.85  # 3 components will explain only 20% of the data variance\n",
        "\n",
        "#df = df.drop('status')\n",
        "#df = df.drop('status_code')\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df[categorical_columns]).toarray()\n",
        "\n",
        "# Create a DataFrame from the encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Create a DataFrame from the scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "principalComponents = pca.fit_transform(processed_df)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Find the number of components that explain at least thereshold_PCA of the variance\n",
        "num_components = np.argmax(cumulative_explained_variance >= thereshold_PCA) + 1\n",
        "\n",
        "print(f'Minimal number of components to explain {thereshold_PCA} variance: {num_components}')\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs. Number of Components')\n",
        "plt.axhline(y=thereshold_PCA, color='r', linestyle='-')\n",
        "plt.axvline(x=num_components-1, color='r', linestyle='-')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Zwhq9i-STY"
      },
      "source": [
        "### PCA with 3 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "28v_18-SDjGY",
        "outputId": "2b78d29f-8592-414e-d8c8-b5dad4c8d12a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Separate the 'status' column\n",
        "status = df['status_code']\n",
        "\n",
        "# Exclude the 'status' column from the features\n",
        "df_pca = df.drop(columns=['status_code'])\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df_pca.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df_pca.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df_pca[categorical_columns]).toarray()\n",
        "\n",
        "# Create a DataFrame from the encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df_pca[numerical_columns])\n",
        "\n",
        "# Create a DataFrame from the scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents = pca.fit_transform(processed_df)\n",
        "\n",
        "# Create a DataFrame with the PCA results and the status\n",
        "pca_df = pd.DataFrame(data=principalComponents, columns=['component_1', 'component_2', 'component_3'])\n",
        "pca_df['status'] = status.values\n",
        "\n",
        "# Plot the first three PCA components in 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for target in pca_df['status'].unique():\n",
        "    subset = pca_df[pca_df['status'] == target]\n",
        "    ax.scatter(subset['component_1'], subset['component_2'], subset['component_3'], label=target, alpha=0.5)\n",
        "ax.set_xlabel('Component 1')\n",
        "ax.set_ylabel('Component 2')\n",
        "ax.set_zlabel('Component 3')\n",
        "ax.set_title('PCA Visualization (First 3 Components)')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8duWCcGkH7_"
      },
      "source": [
        "# TSN-E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "-ocPt25hndI2",
        "outputId": "4abdf634-975f-4613-9b57-3620e96acf3b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "df_tsne = df\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df_tsne.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df_tsne.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df_tsne[categorical_columns]).toarray()\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df_tsne[numerical_columns])\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_data = np.hstack((encoded_categorical, scaled_numerical))\n",
        "\n",
        "# Create a DataFrame with the processed data\n",
        "processed_df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=3, random_state=42)\n",
        "tsne_components = tsne.fit_transform(processed_df)\n",
        "\n",
        "# Create a DataFrame with the t-SNE results and the status\n",
        "tsne_df = pd.DataFrame(data=tsne_components, columns=['component_1', 'component_2', 'component_3'])\n",
        "tsne_df['status'] = status.values\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for target in tsne_df['status'].unique():\n",
        "    subset = tsne_df[tsne_df['status'] == target]\n",
        "    ax.scatter(subset['component_1'], subset['component_2'], subset['component_3'], label=target, alpha=0.5)\n",
        "ax.set_xlabel('Component 1')\n",
        "ax.set_ylabel('Component 2')\n",
        "ax.set_zlabel('Component 3')\n",
        "ax.set_title('t-SNE  Visualization (First 3 Components)')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSRmQPgK0E4M"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o04IhYZ0F-2",
        "outputId": "5af41145-8f23-4786-fa60-033c18683e0c"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6Vo0RUkv05e8",
        "outputId": "4719d8e3-dbe9-4242-b9eb-987c6d6f8191"
      },
      "outputs": [],
      "source": [
        "df[['founded_at', 'first_funding_at', 'last_funding_at', 'state_code.1', 'has_roundC', 'has_roundD']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b3vrtog4NSz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1TzA9xj8D9_"
      },
      "source": [
        "# XGBoost classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omt8fOdwkCAQ"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "QeEz7dO7HYhk",
        "outputId": "576bc415-0365-44dc-bad1-4d61a9d7c59f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = read_and_adjust(file_name = 'startup data.csv')\n",
        "\n",
        "# Separate the 'status_code' column (target) and features\n",
        "X = df.drop(columns=['status_code'])\n",
        "y = df['status_code']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "numerical_columns = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categorical = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Create DataFrame from encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Create DataFrame from scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_df, y, test_size=0.2, random_state=20)\n",
        "\n",
        "# Train an XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and predict probabilities\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "y_prob = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "th = 0.7 #find_optimal_threshold(y_test, y_pred)\n",
        "\n",
        "print(f'optimal threshold is {th}')\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred > th)\n",
        "report = classification_report(y_test, y_pred > th)\n",
        "auc_roc = roc_auc_score(y_test, y_prob)\n",
        "print(f'AUC-ROC: {round(auc_roc, 2)}')\n",
        "print(f'Accuracy: {round(accuracy, 2)}')\n",
        "print(f'Ballanced Accuracy: {round(balanced_accuracy_score(y_test, y_pred > th), 2)}')\n",
        "print('Classification Report:')\n",
        "print(report)\n",
        "show_ConfusionMatrix_test(y_test, y_pred > th)\n",
        "precision, recall = get_precision_and_recall(y_test,  y_pred > th)\n",
        "print(f\"Precision (Test): {precision}\")\n",
        "print(f\"Recall (Test): {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphVd4O1GUgn"
      },
      "source": [
        "## feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "xueMlMHTGWcr",
        "outputId": "335b3566-cdab-4bdb-f57b-16cf4032068a"
      },
      "outputs": [],
      "source": [
        "# Plot top 10 feature importances\n",
        "importance = xgb_clf.feature_importances_\n",
        "feature_names = processed_df.columns\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJVITER6GLTT"
      },
      "source": [
        "## AUC-ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RnoIp3eBGO3L",
        "outputId": "8d22b297-1b74-44e3-e9f4-91523bef217f"
      },
      "outputs": [],
      "source": [
        "# Plot the AUC-ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC (area = {auc_roc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNQCqlvQvPy"
      },
      "source": [
        "## cross - validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCAxjye8SNXB",
        "outputId": "9965bfd8-d5ba-46ae-f695-e1d1dfb6c8a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, make_scorer, balanced_accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = read_and_adjust(file_name = 'startup data.csv')\n",
        "\n",
        "# Separate the 'status_code' column (target) and features\n",
        "X = df.drop(columns=['status_code'])\n",
        "y = df['status_code']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "numerical_columns = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categorical = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Create DataFrame from encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Create DataFrame from scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_df, y, test_size=0.2, random_state=20)\n",
        "\n",
        "# Train an XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'kappa': make_scorer(cohen_kappa_score)\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(xgb_clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
        "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
        "print(f\"Mean Cross-Validation Accuracy: {round(cv_results['test_accuracy'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Balanced Accuracy Scores: {cv_results['test_balanced_accuracy']}\")\n",
        "print(f\"Mean Cross-Validation Balanced Accuracy: {round(cv_results['test_balanced_accuracy'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation AUC Scores: {cv_results['test_roc_auc']}\")\n",
        "print(f\"Mean Cross-Validation AUC: {round(cv_results['test_roc_auc'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
        "print(f\"Mean Cross-Validation Precision: {round(cv_results['test_precision'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Recall Scores: {cv_results['test_recall']}\")\n",
        "print(f\"Mean Cross-Validation Recall: {round(cv_results['test_recall'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation F1 Scores: {cv_results['test_f1']}\")\n",
        "print(f\"Mean Cross-Validation F1: {round(cv_results['test_f1'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Kappa Scores: {cv_results['test_kappa']}\")\n",
        "print(f\"Mean Cross-Validation Kappa: {round(cv_results['test_kappa'].mean(), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz_szxq3JhNM"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6ggQfiJ4Rp"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "status_code = df['status_code']\n",
        "\n",
        "def print_correlations_Spearman_and_Pearson(col1, status_code, text = ''):\n",
        "  # Calculate Spearman and Pearson correlations\n",
        "  spearman_corr, spearman_p_value = spearmanr(col1, status_code)\n",
        "  pearson_corr, pearson_p_value = pearsonr(col1, status_code)\n",
        "\n",
        "  print(f\"Spearman correlation: {spearman_corr}, p-value: {spearman_p_value}\")\n",
        "  print(f\"Pearson correlation: {pearson_corr}, p-value: {pearson_p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSHb_6WJjYz",
        "outputId": "fc00ad62-53e7-4448-990a-4a382ad7e34e"
      },
      "outputs": [],
      "source": [
        "df['category_code_biotech'] = df['category_code'].apply(lambda x: 1 if x == 'biotech' else 0) #only 34 samples with 1\n",
        "print_correlations_Spearman_and_Pearson(df['category_code_biotech'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c57Vo9A6K2dz",
        "outputId": "461049e8-a3a1-4408-a2a4-f44af6b982db"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['relationships'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDwReZDv20V_",
        "outputId": "470235ce-659c-40eb-cbe5-903b77166440"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['avg_participants'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NMLKWM830el",
        "outputId": "7e93fa0c-11b0-4f36-aa5a-a1ed3d60b073"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['has_roundC'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpqub7gt30mn",
        "outputId": "5ec20184-a71e-4bd0-f11b-a4677276f3e5"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['has_roundD'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRyjMRpRmhtO",
        "outputId": "c6a7d92b-3a9d-482e-e0d3-879dd7c41464"
      },
      "outputs": [],
      "source": [
        "# nasdaq is a better prediction feature then inflation\n",
        "print('inflation_at_year_0')\n",
        "print_correlations_Spearman_and_Pearson(df['inflation_at_year_0'], df['status_code'])\n",
        "print('')\n",
        "print('nasdaq_annual_changes_at_year_0')\n",
        "print_correlations_Spearman_and_Pearson(df['nasdaq_annual_changes_at_year_0'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnlgiu8cahDm",
        "outputId": "49f44b6f-a2f6-4fed-ac61-51d978e215c8"
      },
      "outputs": [],
      "source": [
        "#print_correlations_Spearman_and_Pearson(df['gdp_growth_at_year_0'], df['status_code'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
