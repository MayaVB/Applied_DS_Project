{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3sJnuNGjLKp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JMxANwlCZpc"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH2PeWgyCb4d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, balanced_accuracy_score,  classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "def show_ConfusionMatrix_test(y_test, y_test_pred, test_confusion_matrix_title = \"Confusion Matrix (Test)\"):\n",
        "  conf_matrix_log_reg = confusion_matrix(y_test, y_test_pred)\n",
        "  print(\"Confusion Matrix (Test):\")\n",
        "  print(conf_matrix_log_reg)\n",
        "  ConfusionMatrixDisplay(conf_matrix_log_reg).plot()\n",
        "  plt.title(test_confusion_matrix_title)\n",
        "  plt.show()\n",
        "'''\n",
        "def find_optimal_threshold(target, predicted):\n",
        "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
        "    Parameters:\n",
        "    target : Matrix with dependent or target data, where rows are observations\n",
        "    predicted : Matrix with predicted data, where rows are observations\n",
        "\n",
        "    Returns:\n",
        "    list type, with optimal cutoff value\n",
        "    \"\"\"\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    i = np.arange(len(tpr))\n",
        "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
        "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
        "    return list(roc_t['threshold'])\n",
        "'''\n",
        "\n",
        "def get_precision_and_recall(y, y_pred):\n",
        "  return round(precision_score(y, y_pred), 4), round(recall_score(y, y_pred), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiQiTwWWjY1a"
      },
      "source": [
        "# Download dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coIsc_hajY-3",
        "outputId": "dfa74199-b899-459d-9675-1d1b72e16569"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle API\n",
        "# !pip install kaggle\n",
        "\n",
        "# # Make a directory for the Kaggle credentials and move the JSON file there\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# # Set permissions for the JSON file\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !cd ../data/\n",
        "\n",
        "# # Download the dataset\n",
        "# !kaggle datasets download -d manishkc06/startup-success-prediction\n",
        "\n",
        "# # Unzip the downloaded file\n",
        "# !unzip startup-success-prediction.zip\n",
        "\n",
        "# !ls\n",
        "%pip install yfinance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1Rkwo8jhDZ"
      },
      "source": [
        "# Explore and adjust data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jUh80PBqix"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFXKW9TEsb_n"
      },
      "source": [
        "### add economic info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XH9LBT0AW7P"
      },
      "outputs": [],
      "source": [
        "# Affect acquisitions\n",
        "def add_nasdaq_annual_changes(df):\n",
        "    # nasdaq_annual_changes = {\n",
        "    #   1984: -3.66, 1985: 32.99, 1986: 14.82, 1987: 3.53, 1988: 12.06, 1989: 19.20,\n",
        "    #   1990: -17.80, 1991: 56.82, 1992: 15.52, 1993: 14.75, 1994: -3.20, 1995: 39.92,\n",
        "    #   1996: 22.71, 1997: 21.64, 1998: 39.63, 1999: 85.59, 2000: -39.29, 2001: -21.05,\n",
        "    #   2002: -31.53, 2003: 50.01, 2004: 8.59, 2005: 1.37, 2006: 9.52, 2007: 9.81,\n",
        "    #   2008: -40.54, 2009: 43.89, 2010: 16.91, 2011: -1.80, 2012: 15.91, 2013: 38.32,\n",
        "    #   2014: 13.40, 2015: 5.73, 2016: 7.50, 2017: 28.24, 2018: -3.88, 2019: 35.23,\n",
        "    #   2020: 43.64, 2021: 21.39, 2022: -33.10, 2023: 35.03, 2024: 8.00  # 2024 is an estimate\n",
        "    # }\n",
        "    \n",
        "    # Years of interest\n",
        "    years = np.arange(1984, 2024).tolist()\n",
        "\n",
        "    # Download data for the NASDAQ index\n",
        "    nasdaq_data = yf.download('^IXIC', start='1984-01-01', end='2024-01-01', interval='1mo')\n",
        "\n",
        "    # Calculate annual changes\n",
        "    nasdaq_data['Year'] = nasdaq_data.index.year\n",
        "    nasdaq_annual = nasdaq_data.groupby('Year')['Close'].last().pct_change().reset_index()\n",
        "    nasdaq_annual.columns = ['Year', 'NASDAQ_Annual_Change']\n",
        "\n",
        "    # Filter for the years of interest\n",
        "    nasdaq_annual = nasdaq_annual[nasdaq_annual['Year'].isin(years)]\n",
        "\n",
        "    # Convert nasdaq_annual DataFrame to a dictionary\n",
        "    # nasdaq_annual_changes = nasdaq_annual.set_index('Year')['NASDAQ_Annual_Change'].to_dict()\n",
        "\n",
        "    # df['nasdaq_annual_changes_at_year_0'] = df['founded_at_year'].map(nasdaq_annual_changes)\n",
        "\n",
        "    # Merge the datasets\n",
        "    # df = df.merge(nasdaq_annual, left_on='founded_at_year', right_on='Year', how='left')\n",
        "\n",
        "    # Drop the extra 'Year' column after merging\n",
        "    # df = df.drop(columns=['Year'])\n",
        "\n",
        "    # Set the 'Year' column as the index\n",
        "    nasdaq_annual.set_index('Year', inplace=True)\n",
        "\n",
        "    # Create new columns for each year\n",
        "    # for i in range(1, 11):  # For the past 10 years\n",
        "    #     df[f'nasdaq_annual_changes_at_year_{i}'] = df['founded_at_year'].apply(\n",
        "    #         lambda x: nasdaq_annual.get(x - i, np.nan)\n",
        "    #     )\n",
        "\n",
        "    df['nasdaq_annual_changes_at_year_0'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x, None))\n",
        "    df['nasdaq_annual_changes_at_year_1'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-1, None))\n",
        "    df['nasdaq_annual_changes_at_year_2'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-2, None))\n",
        "    df['nasdaq_annual_changes_at_year_3'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-3, None))\n",
        "    df['nasdaq_annual_changes_at_year_4'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-4, None))\n",
        "    df['nasdaq_annual_changes_at_year_5'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-5, None))\n",
        "    df['nasdaq_annual_changes_at_year_6'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-6, None))\n",
        "    df['nasdaq_annual_changes_at_year_7'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-7, None))\n",
        "    df['nasdaq_annual_changes_at_year_8'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-8, None))\n",
        "    df['nasdaq_annual_changes_at_year_9'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-9, None))\n",
        "    df['nasdaq_annual_changes_at_year_10'] = df.founded_at_year.apply(lambda x: nasdaq_annual.get(x-10, None))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Affect workforce and investments\n",
        "def add_info_on_inflation_over_years(df):\n",
        "  us_inflation_data = {\n",
        "      1984: 4.3, 1985: 3.6, 1986: 1.9, 1987: 3.6, 1988: 4.1, 1989: 4.8,\n",
        "      1990: 5.4, 1991: 4.2, 1992: 3.0, 1993: 2.7, 1994: 2.6, 1995: 2.8,\n",
        "      1996: 3.0, 1997: 2.3, 1998: 1.6, 1999: 2.2, 2000: 3.4, 2001: 2.8,\n",
        "      2002: 1.6, 2003: 2.3, 2004: 2.7, 2005: 3.4, 2006: 3.2, 2007: 2.8,\n",
        "      2008: 3.8, 2009: -0.4, 2010: 1.6, 2011: 3.2, 2012: 2.1, 2013: 1.5,\n",
        "      2014: 1.6, 2015: 0.1, 2016: 1.3, 2017: 2.1, 2018: 2.4, 2019: 1.8,\n",
        "      2020: 1.2, 2021: 4.7, 2022: 8.0, 2023: 3.2, 2024: 2.5  # 2024 is an estimate\n",
        "  }\n",
        "\n",
        "  df['inflation_at_year_0'] = df['founded_at_year'].map(us_inflation_data)\n",
        "  df['inflation_at_year_1'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-1, None))\n",
        "  df['inflation_at_year_2'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-2, None))\n",
        "  df['inflation_at_year_3'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-3, None))\n",
        "\n",
        "  df['inflation_at_year_4'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-4, None))\n",
        "  df['inflation_at_year_5'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-5, None))\n",
        "  df['inflation_at_year_6'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-6, None))\n",
        "  df['inflation_at_year_7'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-7, None))\n",
        "  df['inflation_at_year_8'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-8, None))\n",
        "  df['inflation_at_year_9'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-9, None))\n",
        "  df['inflation_at_year_10'] = df.founded_at_year.apply(lambda x: us_inflation_data.get(x-10, None))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Affect workforce and ivestments\n",
        "def add_us_gdp_growth_data_over_years(df):\n",
        "    us_gdp_growth_data = {\n",
        "        1984: 7.3, 1985: 4.2, 1986: 3.5, 1987: 3.5, 1988: 4.2, 1989: 3.7,\n",
        "        1990: 1.9, 1991: -0.1, 1992: 3.6, 1993: 2.8, 1994: 4.0, 1995: 2.7,\n",
        "        1996: 3.8, 1997: 4.5, 1998: 4.5, 1999: 4.8, 2000: 4.1, 2001: 1.0,\n",
        "        2002: 1.7, 2003: 2.9, 2004: 3.8, 2005: 3.5, 2006: 2.9, 2007: 1.9,\n",
        "        2008: -0.1, 2009: -2.5, 2010: 2.6, 2011: 1.6, 2012: 2.2, 2013: 1.8,\n",
        "        2014: 2.5, 2015: 3.1, 2016: 1.6, 2017: 2.4, 2018: 2.9, 2019: 2.3,\n",
        "        2020: -3.4, 2021: 5.9, 2022: 2.1, 2023: 2.0, 2024: 2.3  # 2024 is an estimate\n",
        "    }\n",
        "\n",
        "    df['gdp_growth_at_year_0'] = df['founded_at_year'].map(us_gdp_growth_data)\n",
        "    df['gdp_growth_at_year_1'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-1, None))\n",
        "    df['gdp_growth_at_year_2'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-2, None))\n",
        "    df['gdp_growth_at_year_3'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-3, None))\n",
        "\n",
        "    df['gdp_growth_at_year_4'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-4, None))\n",
        "    df['gdp_growth_at_year_5'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-5, None))\n",
        "    df['gdp_growth_at_year_6'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-6, None))\n",
        "    df['gdp_growth_at_year_7'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-7, None))\n",
        "    df['gdp_growth_at_year_8'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-8, None))\n",
        "    df['gdp_growth_at_year_9'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-9, None))\n",
        "    df['gdp_growth_at_year_10'] = df.founded_at_year.apply(lambda x: us_gdp_growth_data.get(x-10, None))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Affect workforce\n",
        "def add_us_workforce_growth_data_over_years(df):\n",
        "    us_workforce_growth_data = {\n",
        "        1984: 1.5, 1985: 1.6, 1986: 1.5, 1987: 1.7, 1988: 1.6, 1989: 1.4,\n",
        "        1990: 1.3, 1991: 1.1, 1992: 1.3, 1993: 1.2, 1994: 1.4, 1995: 1.2,\n",
        "        1996: 1.3, 1997: 1.2, 1998: 1.3, 1999: 1.2, 2000: 1.2, 2001: 0.9,\n",
        "        2002: 0.8, 2003: 0.7, 2004: 1.1, 2005: 1.2, 2006: 1.1, 2007: 1.1,\n",
        "        2008: 0.7, 2009: -0.3, 2010: 0.7, 2011: 0.7, 2012: 1.0, 2013: 0.9,\n",
        "        2014: 1.0, 2015: 1.2, 2016: 1.1, 2017: 1.0, 2018: 1.3, 2019: 1.0,\n",
        "        2020: -2.3, 2021: 2.5, 2022: 1.8, 2023: 1.0, 2024: 1.1  # 2024 is an estimate\n",
        "    }\n",
        "\n",
        "    df['workforce_growth_at_year_0'] = df['founded_at_year'].map(us_workforce_growth_data)\n",
        "    df['workforce_growth_at_year_1'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-1, None))\n",
        "    df['workforce_growth_at_year_2'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-2, None))\n",
        "    df['workforce_growth_at_year_3'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-3, None))\n",
        "\n",
        "    df['workforce_growth_at_year_4'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-4, None))\n",
        "    df['workforce_growth_at_year_5'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-5, None))\n",
        "    df['workforce_growth_at_year_6'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-6, None))\n",
        "    df['workforce_growth_at_year_7'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-7, None))\n",
        "    df['workforce_growth_at_year_8'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-8, None))\n",
        "    df['workforce_growth_at_year_9'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-9, None))\n",
        "    df['workforce_growth_at_year_10'] = df.founded_at_year.apply(lambda x: us_workforce_growth_data.get(x-10, None))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6dM8K_zsoVT"
      },
      "source": [
        "### add & adjust data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ficszZWssKj"
      },
      "outputs": [],
      "source": [
        "def read_and_adjust(file_name = 'startup data.csv'):\n",
        "  # Load the data\n",
        "  df = pd.read_csv(file_name)\n",
        "\n",
        "  # Create lable\n",
        "  df['status_code'] = df['status'].map({'acquired': 1, 'closed': 0})\n",
        "\n",
        "  # Convert the date column to datetime\n",
        "  df['founded_at_date'] = pd.to_datetime(df['founded_at'])\n",
        "\n",
        "  # Extract year, month, and day into separate columns\n",
        "  df['founded_at_year'] = df['founded_at_date'].dt.year\n",
        "  df['founded_at_month'] = df['founded_at_date'].dt.month\n",
        "  df['founded_at_day'] = df['founded_at_date'].dt.day\n",
        "\n",
        "  # Drop colums\n",
        "  df = df.drop(columns=['status', 'founded_at', 'name', 'id', 'state_code', 'object_id', 'category_code', 'labels', 'closed_at'])\n",
        "\n",
        "  #df = add_info_on_inflation_over_years(df)\n",
        "  df = add_us_gdp_growth_data_over_years(df)\n",
        "  #df = add_us_workforce_growth_data_over_years(df)\n",
        "  df = add_nasdaq_annual_changes(df)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B2BTdJJBtyN"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7W7KnxJjljM",
        "outputId": "7b58047f-a380-47d3-f6dc-21c04d6e4839"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "dataset_path = '../data/startup_data.csv'\n",
        "df = read_and_adjust(file_name = dataset_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfCxLxAw-1aJ",
        "outputId": "6d8d595b-7f17-4794-9b3b-141b6b93aee0"
      },
      "outputs": [],
      "source": [
        "df.founded_at_year.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbNsKVg5jhOM",
        "outputId": "52c67b88-cf3a-48de-e3af-72b738667c94"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFnaN8kKj9zQ",
        "outputId": "7928b7c6-742a-418a-f787-a9512d3a0a64"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtaYf4umO6L"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06pLkTuZEoR"
      },
      "source": [
        "### Number of components needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "3c_12tPz69Pv",
        "outputId": "5dd63f51-2227-4894-904f-b58c15bd42ac"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "thereshold_PCA = 0.85  # 3 components will explain only 20% of the data variance\n",
        "\n",
        "#df = df.drop('status')\n",
        "#df = df.drop('status_code')\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df[categorical_columns]).toarray()\n",
        "\n",
        "# Create a DataFrame from the encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Create a DataFrame from the scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "principalComponents = pca.fit_transform(processed_df)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Find the number of components that explain at least thereshold_PCA of the variance\n",
        "num_components = np.argmax(cumulative_explained_variance >= thereshold_PCA) + 1\n",
        "\n",
        "print(f'Minimal number of components to explain {thereshold_PCA} variance: {num_components}')\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs. Number of Components')\n",
        "plt.axhline(y=thereshold_PCA, color='r', linestyle='-')\n",
        "plt.axvline(x=num_components-1, color='r', linestyle='-')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Zwhq9i-STY"
      },
      "source": [
        "### PCA with 3 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "28v_18-SDjGY",
        "outputId": "2b78d29f-8592-414e-d8c8-b5dad4c8d12a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Separate the 'status' column\n",
        "status = df['status_code']\n",
        "\n",
        "# Exclude the 'status' column from the features\n",
        "df_pca = df.drop(columns=['status_code'])\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df_pca.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df_pca.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df_pca[categorical_columns]).toarray()\n",
        "\n",
        "# Create a DataFrame from the encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df_pca[numerical_columns])\n",
        "\n",
        "# Create a DataFrame from the scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents = pca.fit_transform(processed_df)\n",
        "\n",
        "# Create a DataFrame with the PCA results and the status\n",
        "pca_df = pd.DataFrame(data=principalComponents, columns=['component_1', 'component_2', 'component_3'])\n",
        "pca_df['status'] = status.values\n",
        "\n",
        "# Plot the first three PCA components in 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for target in pca_df['status'].unique():\n",
        "    subset = pca_df[pca_df['status'] == target]\n",
        "    ax.scatter(subset['component_1'], subset['component_2'], subset['component_3'], label=target, alpha=0.5)\n",
        "ax.set_xlabel('Component 1')\n",
        "ax.set_ylabel('Component 2')\n",
        "ax.set_zlabel('Component 3')\n",
        "ax.set_title('PCA Visualization (First 3 Components)')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8duWCcGkH7_"
      },
      "source": [
        "# TSN-E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "-ocPt25hndI2",
        "outputId": "4abdf634-975f-4613-9b57-3620e96acf3b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "df_tsne = df\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = df_tsne.select_dtypes(include=['object']).columns\n",
        "numerical_columns = df_tsne.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "encoded_categorical = encoder.fit_transform(df_tsne[categorical_columns]).toarray()\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(df_tsne[numerical_columns])\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_data = np.hstack((encoded_categorical, scaled_numerical))\n",
        "\n",
        "# Create a DataFrame with the processed data\n",
        "processed_df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=3, random_state=42)\n",
        "tsne_components = tsne.fit_transform(processed_df)\n",
        "\n",
        "# Create a DataFrame with the t-SNE results and the status\n",
        "tsne_df = pd.DataFrame(data=tsne_components, columns=['component_1', 'component_2', 'component_3'])\n",
        "tsne_df['status'] = status.values\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for target in tsne_df['status'].unique():\n",
        "    subset = tsne_df[tsne_df['status'] == target]\n",
        "    ax.scatter(subset['component_1'], subset['component_2'], subset['component_3'], label=target, alpha=0.5)\n",
        "ax.set_xlabel('Component 1')\n",
        "ax.set_ylabel('Component 2')\n",
        "ax.set_zlabel('Component 3')\n",
        "ax.set_title('t-SNE  Visualization (First 3 Components)')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSRmQPgK0E4M"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o04IhYZ0F-2",
        "outputId": "5af41145-8f23-4786-fa60-033c18683e0c"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6Vo0RUkv05e8",
        "outputId": "4719d8e3-dbe9-4242-b9eb-987c6d6f8191"
      },
      "outputs": [],
      "source": [
        "df[['founded_at', 'first_funding_at', 'last_funding_at', 'state_code.1', 'has_roundC', 'has_roundD']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b3vrtog4NSz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1TzA9xj8D9_"
      },
      "source": [
        "# XGBoost classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omt8fOdwkCAQ"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "QeEz7dO7HYhk",
        "outputId": "576bc415-0365-44dc-bad1-4d61a9d7c59f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = read_and_adjust(file_name = 'startup data.csv')\n",
        "\n",
        "# Separate the 'status_code' column (target) and features\n",
        "X = df.drop(columns=['status_code'])\n",
        "y = df['status_code']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "numerical_columns = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categorical = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Create DataFrame from encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Create DataFrame from scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_df, y, test_size=0.2, random_state=20)\n",
        "\n",
        "# Train an XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and predict probabilities\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "y_prob = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "th = 0.7 #find_optimal_threshold(y_test, y_pred)\n",
        "\n",
        "print(f'optimal threshold is {th}')\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred > th)\n",
        "report = classification_report(y_test, y_pred > th)\n",
        "auc_roc = roc_auc_score(y_test, y_prob)\n",
        "print(f'AUC-ROC: {round(auc_roc, 2)}')\n",
        "print(f'Accuracy: {round(accuracy, 2)}')\n",
        "print(f'Ballanced Accuracy: {round(balanced_accuracy_score(y_test, y_pred > th), 2)}')\n",
        "print('Classification Report:')\n",
        "print(report)\n",
        "show_ConfusionMatrix_test(y_test, y_pred > th)\n",
        "precision, recall = get_precision_and_recall(y_test,  y_pred > th)\n",
        "print(f\"Precision (Test): {precision}\")\n",
        "print(f\"Recall (Test): {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphVd4O1GUgn"
      },
      "source": [
        "## feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "xueMlMHTGWcr",
        "outputId": "335b3566-cdab-4bdb-f57b-16cf4032068a"
      },
      "outputs": [],
      "source": [
        "# Plot top 10 feature importances\n",
        "importance = xgb_clf.feature_importances_\n",
        "feature_names = processed_df.columns\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJVITER6GLTT"
      },
      "source": [
        "## AUC-ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RnoIp3eBGO3L",
        "outputId": "8d22b297-1b74-44e3-e9f4-91523bef217f"
      },
      "outputs": [],
      "source": [
        "# Plot the AUC-ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC (area = {auc_roc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNQCqlvQvPy"
      },
      "source": [
        "## cross - validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCAxjye8SNXB",
        "outputId": "9965bfd8-d5ba-46ae-f695-e1d1dfb6c8a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, make_scorer, balanced_accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = read_and_adjust(file_name = 'startup data.csv')\n",
        "\n",
        "# Separate the 'status_code' column (target) and features\n",
        "X = df.drop(columns=['status_code'])\n",
        "y = df['status_code']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "numerical_columns = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "# OneHotEncode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categorical = encoder.fit_transform(X[categorical_columns])\n",
        "\n",
        "# Create DataFrame from encoded categorical data\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(X[numerical_columns])\n",
        "\n",
        "# Create DataFrame from scaled numerical data\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical data\n",
        "processed_df = pd.concat([encoded_categorical_df, scaled_numerical_df], axis=1)\n",
        "\n",
        "# Replace NaN values with the average value of their respective columns\n",
        "processed_df.fillna(processed_df.mean(), inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_df, y, test_size=0.2, random_state=20)\n",
        "\n",
        "# Train an XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'kappa': make_scorer(cohen_kappa_score)\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(xgb_clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
        "print(f\"Cross-Validation Accuracy Scores: {cv_results['test_accuracy']}\")\n",
        "print(f\"Mean Cross-Validation Accuracy: {round(cv_results['test_accuracy'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Balanced Accuracy Scores: {cv_results['test_balanced_accuracy']}\")\n",
        "print(f\"Mean Cross-Validation Balanced Accuracy: {round(cv_results['test_balanced_accuracy'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation AUC Scores: {cv_results['test_roc_auc']}\")\n",
        "print(f\"Mean Cross-Validation AUC: {round(cv_results['test_roc_auc'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Precision Scores: {cv_results['test_precision']}\")\n",
        "print(f\"Mean Cross-Validation Precision: {round(cv_results['test_precision'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Recall Scores: {cv_results['test_recall']}\")\n",
        "print(f\"Mean Cross-Validation Recall: {round(cv_results['test_recall'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation F1 Scores: {cv_results['test_f1']}\")\n",
        "print(f\"Mean Cross-Validation F1: {round(cv_results['test_f1'].mean(), 2)}\")\n",
        "print(f\"Cross-Validation Kappa Scores: {cv_results['test_kappa']}\")\n",
        "print(f\"Mean Cross-Validation Kappa: {round(cv_results['test_kappa'].mean(), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz_szxq3JhNM"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6ggQfiJ4Rp"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "status_code = df['status_code']\n",
        "\n",
        "def print_correlations_Spearman_and_Pearson(col1, status_code, text = ''):\n",
        "  # Calculate Spearman and Pearson correlations\n",
        "  spearman_corr, spearman_p_value = spearmanr(col1, status_code)\n",
        "  pearson_corr, pearson_p_value = pearsonr(col1, status_code)\n",
        "\n",
        "  print(f\"Spearman correlation: {spearman_corr}, p-value: {spearman_p_value}\")\n",
        "  print(f\"Pearson correlation: {pearson_corr}, p-value: {pearson_p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSHb_6WJjYz",
        "outputId": "fc00ad62-53e7-4448-990a-4a382ad7e34e"
      },
      "outputs": [],
      "source": [
        "df['category_code_biotech'] = df['category_code'].apply(lambda x: 1 if x == 'biotech' else 0) #only 34 samples with 1\n",
        "print_correlations_Spearman_and_Pearson(df['category_code_biotech'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c57Vo9A6K2dz",
        "outputId": "461049e8-a3a1-4408-a2a4-f44af6b982db"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['relationships'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDwReZDv20V_",
        "outputId": "470235ce-659c-40eb-cbe5-903b77166440"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['avg_participants'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NMLKWM830el",
        "outputId": "7e93fa0c-11b0-4f36-aa5a-a1ed3d60b073"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['has_roundC'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpqub7gt30mn",
        "outputId": "5ec20184-a71e-4bd0-f11b-a4677276f3e5"
      },
      "outputs": [],
      "source": [
        "print_correlations_Spearman_and_Pearson(df['has_roundD'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRyjMRpRmhtO",
        "outputId": "c6a7d92b-3a9d-482e-e0d3-879dd7c41464"
      },
      "outputs": [],
      "source": [
        "# nasdaq is a better prediction feature then inflation\n",
        "print('inflation_at_year_0')\n",
        "print_correlations_Spearman_and_Pearson(df['inflation_at_year_0'], df['status_code'])\n",
        "print('')\n",
        "print('nasdaq_annual_changes_at_year_0')\n",
        "print_correlations_Spearman_and_Pearson(df['nasdaq_annual_changes_at_year_0'], df['status_code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnlgiu8cahDm",
        "outputId": "49f44b6f-a2f6-4fed-ac61-51d978e215c8"
      },
      "outputs": [],
      "source": [
        "#print_correlations_Spearman_and_Pearson(df['gdp_growth_at_year_0'], df['status_code'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
